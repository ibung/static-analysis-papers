\documentclass[a4paper,UKenglish]{lipics-v2016}
% This template is based on the template for producing LIPIcs articles.
% See lipics-manual.pdf for further information.
% https://www.dagstuhl.de/en/publications/lipics/instructions-for-authors/
% for section-numbered lemmas etc., use "numberwithinsect"

% we do not need the copyright line or the DOI
\renewcommand{\copyrightline}{}
\DOIPrefix{}

\bibliographystyle{plainurl}% the recommended bibstyle

% Author macros::begin %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Advanced Type-Based Analysis: Final Report}

%% Add all author names here
\author[1]{Nikita Ziuzin}
\author[2]{Joachim Bard}
\author[3]{Hizbullah Abdul Aziz}
% List email addresses in the same order as the authors
\affil[1]{\texttt{nzyuzin93@gmail.com}}
\affil[2]{\texttt{s9jobard@stud.uni-saarland.de}}
\affil[3]{\texttt{s8hijabb@stud.uni-saarland.de}}

\begin{document}

\maketitle


\section{Introduction}
\label{sec:intro}

  Many programming languages have some sort of type system that enforces a
  certain notion of correctness from a program.  For example, a function that
  adds two numbers, when defined correctly, should only works on numbers and
  nothing else (which are reflected on the type of its arguments).  A
  typechecker ensures that a program is well-typed, that is, everything has a
  well-defined type, by statically checking the type of every part of a program
  (usually) during compile time.  In this particular example, trying to call
  the addition function on a number and a boolean valua should make the
  typechecker rejects the program as not well-typed.

  However, well-typed programs can still go wrong.  Standard type systems found
  in mainstream programming language such as C, Java, or even the more advanced
  ones in ML or Haskell can't prevent some bad runtime behaviors such as
  division by zero.  Thus there have been some research on extending types and
  type systems to become more expressive.  In particular, there is a notion of
  Dependent Types, which are types that depend on some values.  Starting with
  statically checking array bound as a motivating example, Dependent Types
  have spawned many subsequent researches, two of which will be presented in
  this report.

  In this summary, we will begin with the high-level summary Dependent Types
  itself~\cite{Xi:1998:EAB}.  We will then proceed with a brief explanation of
  Liquid Types~\cite{Rondon:2008:LT}, which is types \emph{refined} with
  predicates that builds on the ideas of Dependent Types.  In particular,
  Liquid Types make it possible to reduce the number of annotation needed while
  at the same time retains the benefit of Dependent Types.  Finally, we
  summarizes the approach taken to bring Liquid Types under lazy evaluation
  settings~\cite{Vazou:ICFP:2014}.

  % This is from of the template
  In high-level terms
  \begin{itemize}
    \item introduce the topic

    \item explain why it is relevant/related to the course (i.e. how is it related
    to static analysis)

    \item explain why the topic is important (e.g. where and how can it be used)

    \item introduce your chosen papers and briefly explain how they are related
    to the topic
  \end{itemize}

\section{Summary of Papers}

\subsection{Paper 1: Eliminating array bound checking through dependent types}

The problem of array bound checking is persistent in many programming
languages. That is, the languages that are said to be memory-safe in the
runtime such as \textbf{ML} or \textbf{Java} need to include the checks for
array access in the compiled code resulting in the increased execution time
costs when the checking is redundant. On the other hand, the languages that do
not employ array bound checking in runtime, such as \textbf{C} result in
particularly unpleasant runtime bugs when the programmer is not careful.

To address this issue, several methods have been developed over time. Many of
such methods try to eliminate redundant checks without any help from the
programmer, through generation of loop invariants which is not generally
decidable in theory and may be difficult in practice.

A possible approach to solving this problem is to use dependent types. This
way, array bound checking is performed during the type checking through an
integer constraint validation. Naturally, that brings no additional cost for
the runtime.  However, this approach requires some help from the programmer,
namely, correct type annotations for the program. The following sections will
concern the implementation and application for the idea.

\subsubsection{Dependent types}

The concept of the dependent types is quite rich and well-studied, the main
idea behind it is that dependent type systems unlike systems with simple types
allow for types to depend on or vary with values. Examples to simple types are
products, functions or atomic types that are \texttt{int} or \texttt{bool}.

A classical example of a dependent type is a type of lists $List_\gamma(N)$ of
a given length $N : \mathbb{N}$ over some type $\gamma$. The elements of such
type are $Nil_\gamma : List(0)$ and $Cons_\gamma(x, l) : List(N+1)$, where $x :
\gamma$ and $l : List_\gamma(N)$. Now let's consider a function that takes $n :
\mathbb{N}$ and returns a list of length $n$ containing $0$ in all entries.
This function would have type $\Pi n : \mathbb{N}.List_\mathbb{N}(n)$, a type
of functions for which the type of the return value depends on the argument.
The important point of the dependent typing is that it reveals more information
about the function.

It's worth notice that the general definition of the dependent types allows for
terms of any kind to be used in the typing expression. Naturally, this makes
type checking equivalent to an execution of an arbitrary program, which in
turns makes it undecidable in general.

To avoid this problem a restricted form of dependent types is required that
will allow to remain decidable and at the same time be expressive enough to
solve the problem at hand.

For a full study on the dependent types the reader is referred to
\cite{Hofmann97syntaxand}.

\subsubsection{Restriction on dependent types}

\textbf{DML}, a language that covers an \textbf{ML} fragment extended with a
limited form of dependent types is used to give a practical validation for the
solution. This fragment matches the core language of the \textbf{ML} standard,
making omission for exceptions and modules, and contains the addition of the
restricted form for the dependent type annotations.

This form is consists of linear integer arithmetic (LIA) and boolean
expressions that can be solved by a LIA solver.

In the concrete syntax a value on which a dependent type is formed will be
called an \emph{index object}. For example, a type of integer lists in
\textbf{ML} would be written as follows: \texttt{int list}. To express a type
of the integer lists of length two one writes: \texttt{int list(2)}, where 2 is
exactly the \emph{index object}. Then a function to append two integer lists of
some fixed lengths \texttt{n} and \texttt{m} would have the type:
\texttt{int(n) -> int(m) -> int(n+m)}.

Index objects can be boolean constants $false$, $true$ or boolean expressions
composed by $\land$, $\lor$, $\neg$, and integer expressions resulting in
either boolean or integer that are composed with the usual operations on
integers: $+$, $-$, $*$, $div$, $min$, $max$, $abs$, $sign$, $mod$, $>$, $<$,
$\ge$, $\le$, $=$.

To quantify the variables used in the index object one writes \texttt{\{a:g\}
t} for the universal quantification (often written in the literature as $\Pi a:
\gamma.  \tau$) and \texttt{[a:g] t} for the existential quantification
($\Sigma a: \gamma. \tau$).  $\{ a: g | b \}$ corresponds to those elements of
a type $g$ that satisfy the boolean constraints $b$. There's also a shorthand
\texttt{\{a:g | b\} t} for $\Pi a: \{a: \gamma | b\}. \tau$. And then $nat$
serves as an abbreviation for $\{ a: int | a \geq 0 \}$.

\begin{lstlisting}[caption={The filter function for lists},label=filter,captionpos=t,float,abovecaptionskip=-\medskipamount]
fun filter p nil = nil
  | filter p (x::xs) =
    if p(x) then x::(filter p xs)
    else filter p xs
where filter <| {m: nat}
  ('a -> bool) -> 'a list(m) -> [n:nat | n <= m] 'a list(n)
\end{lstlisting}

Figure \ref{filter} serves as an example program. It depicts a function that
given a list over some type $A$ and a predicate on $A$ returns a list
containing all the elements that satisfy the predicate. The dependent types
annotation, \texttt{where filter <| \{m: nat\} ('a -> bool) -> 'a list(m) ->
[n:nat | n <= m] 'a list(n)} displays the relation between input and output
lists, that is that the resulting list is shorter than the initial.

\begin{lstlisting}[caption={The reverse function for lists},label=reverse,captionpos=t,float,abovecaptionskip=-\medskipamount]
fun reverse(l) =
let
  fun rev(nil, ys) = ys
    | rev(x::xs, ys) = rev(xs, x::ys)
  where rev <| {m:nat} {n:nat}
    'a list(m) * 'a list(n) -> 'a list(m+n)
in
  rev(l, nil)
end
where reverse <| {n:nat} 'a list(n) -> 'a list(n)
\end{lstlisting}

For another example consider the function that returns a reverse for a given
list. Figure \ref{reverse} displays the implementation of such a function in
\textbf{DML}. The type here ensures that the size of the list doesn't change
from the argument to the result.

\subsubsection{Type checking on the restriction}

The first step of type checking in \textbf{DML} is to ignore index objects and
ensure that the program type checks without them as an \textbf{ML} program.
Afterwards, index objects are checked. Since the allowed expressions form
linear integer inequalities, constraints on the integers can be extracted form
the index objects and checked separately.

Consider for the example the \texttt{rev} function from the Figure
\ref{reverse}:
\begin{verbatim}
fun rev(nil, ys) = ys
  | rev(x::xs, ys) = rev(xs, x::ys)
where rev <| {m:nat} {n:nat}
  'a list(m) * 'a list(n) -> 'a list(m+n)
\end{verbatim}

It uses pattern matching as a function definition and thus one can consider the
two cases of \texttt{rev} separately. For the first case, \texttt{rev(nil, ys)
= ys} the two variables $M$ and $N$ are generated according to the index
objects of the function type. Then \texttt{nil} is checked against the type
\texttt{'a list($M$)} and \texttt{ys} is checked against the type \texttt{'a
list($N$)}.

This generates the constraints $M = 0$ and $N = \texttt{n}$, where \texttt{ys}
is assumed to be of type \texttt{'a list(n)}. Afterwards, the type of the right
hand side of the pattern matching clause, \texttt{ys}, is checked against
\texttt{'a list($M$ + $N$)}, the result type for \texttt{rev}. This yields a
constraint $M + N = \texttt{n}$.

Thus, the first clause of the pattern matching in the function definition
generates the constraint:

$\forall n : nat. (0 + n = n)$

Analogously for the second case, \texttt{rev(x::xs, ys) = rev(xs, x::ys)}:

$\forall m : nat. \forall n : nat.(m + 1) + n = m + (n + 1)$,

Such constraints are then solved with a LIA-solver and if they are found
satisfiable then the type checking succeeds.

\subsubsection{Evaluation}

The use of the dependent types allows the compiler to know the length of all
properly type annotated lists and arrays in the program. Because of that, the
runtime array bound checks become redundant and the compiler may safely omit
them.

Under a set of benchmarks that use the programs taken from a few
\textbf{Standard ML} implementations standard libraries, the measurements
display the considerable performance gain of average 30\% among all programs.
These benefits come at the cost of changing the type annotations to include
dependent types which take about 17\% of lines in the programs.

\subsection{Paper 2: Liquid Types}

\subsubsection{Goal/Problem}

\subsubsection{Main Idea}

Rondon et al. introduced logically qualified data types or short Liquid types in~\cite{Rondon:2008:LT}.
Liquid types are dependent types and thus one can express types like $i:: \{\nu:$ \lstinline{int} $\mid 0 \le \nu \land \nu < 100\}$.
If one thinks of $i$ as an index of an array with 100 elements the type ensures that one can safely access the array at index $i$.
The advantage of liquid types is that those types can be infered automatically.
The inference is done in 3 steps:
\begin{enumerate}
    \item Step 1 performs a classical type inference.
        The obtained types are transformed into templates where the unknown type refinements are expressed by so called liquid type variables.
    \item Step 2 generates a set of subtype constraints which restrict choices for the liquid type variables.
        This step is guided by the syntax.
    \item Step 3 tries to solve these constraints by embedding them into a decidable logic.
\end{enumerate}

To describe the refinements of liquid types we take a set of predicates (qualifiers) over program variables and a special variable $\nu$.
By taking conjunctions of qualifiers from $P \cup \neg P$ we get refinements of liquid types.
A base type $B$ is a short form of $\{\nu: B \mid true\}$.
We write $\kappa$ for $\{\nu: B \mid \kappa\}$ with a liquid type variable $\kappa$ if the base type $B$ is clear from the context.
%how to choose $P$

\begin{lstlisting}[caption={Example Program},label=lst:exmpl,captionpos=t,float,abovecaptionskip=-\medskipamount]
let max x y =
    if x > y then x else y
\end{lstlisting}

%path sensitive
We will show that the function \lstinline{max} in Listing~\ref{lst:exmpl} returns a value which is greater or equal to both arguments.
In step 1 classical type inference yields the type $x:$ \lstinline{int} $\rightarrow y:$ \lstinline{int} $\rightarrow$ \lstinline{int} for \lstinline{max}.
Using this type we build the template $x: \kappa_x \rightarrow y: \kappa_y \rightarrow \kappa$.
The liquid type variables $\kappa_x, \kappa_y, \kappa$ are the unknown refinements for the arguments $x$ and $y$ plus the body of \lstinline{max}.
Step 2 generates the following constraints since the body is an \lstinline{if}-expression:
\begin{align}
    \label{algn:max}
    x: \kappa_x; y: \kappa_y; (x > y) \vdash \{\nu: \texttt{int} \mid \nu = x \} <: \kappa\\
    x: \kappa_x; y: \kappa_y; \neg(x > y) \vdash \{\nu: \texttt{int} \mid \nu = y \} <: \kappa
\end{align}
The first constraint forces the type of the \lstinline{then}-branch (namely $\{\nu: \texttt{int} \mid \nu = x \}$) to be a subtype
of the body ($\kappa$) in the environment where $x$ and $y$ have the appropriate types as well as the branch condition is true.
The second contraint is analogous.
Note that in the environment the branch condition is false.
So path sensitivity is introduced by adding assumptions to the environment.
In step 3 we set $\kappa_x$ and $\kappa_y$ to true because \lstinline{max} can be called by any integer.
Moreover we use a theorem prover to get the strongest conjunction of qualifiers in $\mathbb{Q}^\star$ for $\kappa$ which staisfies the two constraints.
We get $x \le \nu \land y \le \nu$ since if $x > y \land \nu = x$ then $x \le \nu \land y \le \nu$ and if $\neg(x > y) \land \nu = y$
then $x \le \nu \land y \le \nu$.
Thus we obtain the type:
\begin{center}
    \lstinline{max} :: $x:$ \lstinline{int} $\rightarrow y:$ \lstinline{int} $\rightarrow \{\nu:$ \lstinline{int} $\mid x \le \nu \land y \le \nu\}$.
\end{center}

%recursion
%higher-order functions
%polymorphism and array bounds checking

\subsubsection{Details}

The liquid type checking is formalized in a $\lambda$-calculus with \lstinline{if}- and \lstinline{let}-expressions, recursive funtions and polymorphism.
Intuitively a liquid type $T$ is well-formed under the environment $\Gamma$ ($\Gamma \vdash T$) if the free variables occuring in the refinements of $T$ are bound in $\Gamma$.
The environment $\Gamma$ consists of type bindings ($x: T$) and predicates.
For example we can say that $\{\nu:$ \lstinline{int} $\mid x \le \nu \land y \le \nu$ is well-formed under $x:$ \lstinline{int}; $y:$ \lstinline{int}.
In addition there is also a decidable subtype judgment $\Gamma \vdash T_1 <: T2$ stating that $T_1$ is a subtype of $T_2$ under environment $\Gamma$.
As example consider $x:$ \lstinline{int}; $y:$ \lstinline{int}; $(x > y) \vdash \{\nu = x \} <: x \le \nu \land y \le \nu$
The liquid type judgement $\Gamma \vdash_{\mathbb{Q}} e: T$ says that the expression $e$ has type $T$ under environment $\Gamma$ if one considers the logical qualifiers $\mathbb{Q}$.
For example we can show $\Gamma \vdash_{\mathbb{Q}}$ \lstinline{let} $s$ \lstinline{= sum (k - 1) in s + k} $: T$ if $T$ is well-formed under $\Gamma$ and the refinements of $T$ consists of conjunctions over logical qualifiers in $\mathbb{Q}^\star$, plus
$\Gamma \vdash_{\mathbb{Q}}$ \lstinline{sum (k - 1)} $:T_1$ and $\Gamma; x: T_1 \vdash_{\mathbb{Q}}$ \lstinline{s + k} $: T$.
Soundness of this liquid type checking is proven in two steps.
Firstly, one defines an exact type system $\Gamma \vdash e: T$ using an undecidable subtype judgment.
Then soundness is obtained using the following theorems, where $\succ$ is the single evaluation step relation:
\begin{enumerate}
    \label{thm:sound}
    \item (Overapproximation) If $\Gamma \vdash_{\mathbb{Q}} e: T$ then $\Gamma \vdash e: T$.
    \item (Preservation) If $\Gamma \vdash e: T$ and $e \succ e'$ then $\Gamma \vdash e': T$.
    \item (Progress) If $\emptyset \vdash e: T$ and $e$ is not a value then $\exists e'. e \succ e'$.
\end{enumerate}

The 3 steps of the type inference can be implemented in the formal setting.
%The examples given in Subsection~\ref{subsec:exmpl} illustrate some cases of this algorithm.
Also for this algorithm called \lstinline{Infer}($\Gamma, e, \mathbb{Q}$) one can show some theorems:
\begin{enumerate}
    \item \lstinline{Infer}($\Gamma, e, \mathbb{Q}$) terminates.
    \item If \lstinline{Infer}$(\Gamma, e, \mathbb{Q} = T$ then $\Gamma \vdash_{\mathbb{Q}} e: T$.
    \item If \lstinline{Infer}($\Gamma, e, \mathbb{Q}$) returns no type then $\nexists T.\, \Gamma \vdash_{\mathbb{Q}} e: T$.
\end{enumerate}

Combining these with the theorems~\ref{thm:sound} we obtain that if \lstinline{Infer}$(\emptyset, e, \mathbb{Q} = T$ then the evalution of $e$ won't get stuck.

\subsubsection{Evaluation}

In order to assess the advantage of this approach liquid type inference is implemented in \textbf{DSolve}.
It is tested over a set of benchmarks from the \textbf{DML} project~\cite{Xi:1998:EAB}.
To allow array bounds checking the qualifiers are of the form $\nu \bowtie X$ where $\bowtie \in \{<, \le, =, \neq, \geq, >\}$ and $X \in \{0, \star,$ \lstinline{len} $\star$\}.
In most cases \textbf{DSolve} can prove the safety of array accesses without additional annotations i.e. qualifiers.
If qualifiers are missing the infered types help to identify which ones should be added.
Compared to the \textbf{DML} project the impact is very big.
Whereas in the \textbf{DML} project 17\% of all lines (31\% of characters) are annotations \textbf{DSolve} only needs 1\% of lines and characters.

  \subsection{Refinement Types For Haskell}

    \subsubsection{Lazy Evaluation}

      Some programming languages, most notably Haskell, use a different
      evaluation strategy called lazy evaluation.  Intuitively, lazy evaluation
      means that computations is only performed if the results of those
      computations are needed.  For example, if a program specifies that the
      result of \texttt{div 10 5} be printed on screen, then the this
      computation i.e. divide 10 by 5 will be computed.  Conversely, if the
      result of \texttt{div 10 5} is never needed, then it will never return a
      value.  

      It turns out that classical approach on refinement types is unsound under
      lazy evaluation, in the sense that the typechecker will admit some bad
      behaviours.  Consider the following example.
      \begin{verbatim}
        type Pos = { v:Int | v >  0 }
        type Nat = { v:Int | v >= 0 }

        div :: n:Nat -> d:Pos -> { v:Nat | v < n } 
        (+) :: x:Int -> y:Int -> { v:Int | v = x + y }
        diverge :: Int -> { v:Nat | false } 

        explode :: Int -> Int
        explode x = let n = diverge 1 in 
                    let y = 0 in 
                    div x y
      \end{verbatim}

      When checking for the type of \texttt{y}, the typechecker will generate
      the constraint
      $$\mathtt{false}\wedge(y=0)\implies(v=0)\implies(v>0)$$
      which any SMT solver will accept since we have falsity in the antecedent.
      Note that the \texttt{false} part in the constraint comes from
      \texttt{diverge}, which essentially states that this function diverges
      i.e. will not return any value.

      This constraint is of course sound under eager evaluation, since we will
      never get into calling \texttt{div} with the second argument equals to
      zero i.e. division by zero.  Under lazy evaluation however, since
      \texttt{n} is not needed in the computation of the result of
      \texttt{explode}, the computation will jump straight into evaluating
      \texttt{div} thus resulting in division by zero.  Recall that the type of
      \texttt{div} should prevent this since it requires its second argument to
      have type \texttt{Pos}, a positive integer, which proves the unsoundness
      of the previous approach.

    \subsubsection{Solution}
      
      To restore soundness, the main idea is to label binders as potentially
      diverging and omit those in the typing constraint.  Returning to the
      previous example, by omitting \texttt{diverge} in the constraint, we have
      $$\mathtt{true}\wedge(y=0)\implies(v=0)\implies(v>0)$$
      which is now an invalid constraint and thus the typechecker will reject
      this as not well-typed.

      One way to implement this is to have a \emph{stratified} type system
      consisting of types which may diverge and those which may not.
      \begin{itemize}
        \item Initially, every binders have types which may diverge.  This is
          sound but very imprecise since now there is no constraint at all.
        \item Then termination analysis is used to determine which binders
          actually terminate, thus getting some constraints back.
      \end{itemize}

      Now it should be clear that the accuracy of this approach depends on the
      termination analysis.  Loosely speaking, there are three main points of
      it.
      \begin{itemize}
        \item As with previous approaches, the refinements itself must be
          restricted into some sort of decidable logic, because otherwise
          typechecking would be undecidable.  Here refinements are drawn from
          QF-EUFLIA, the decidable logic of equality, uninterpreted
          functions and linear arithmetic.
        \item To prove termination of inductively defined functions i.e.
          recursive functions, it employs some default size measures and check
          whether those measures decrease on each (recursive) function call.
          This approach is of course not unlike one found in other programming
          languages such as Coq. Using these default measures, termination can
          be proven automatically.
        \item If the automatic termination analysis fails, then the size
          measure must be explicitly specified by the user.
      \end{itemize}

    \subsection{Implementation and Result}
      
      LIQUIDHASKELL, a Haskell extension with refinement types succesfully
      brings refinement typechecking under lazy evaluation.  The termination
      analysis used in LIQUIDHASKELL is usable since it only requires about 1
      line of code of explicit size measure per 100 line of code, thus
      retaining the advantage of previous approach while maintaining soundness
      under lazy evaluation.

\section{Context and Connection (feel free to change the section names)}

  As briefly mentioned in section~\ref{sec:intro}, Dependent Types and its
  subsequent improvements are extensions to the standard type systems discussed
  in the class.  These extensions make the type system more expressive and thus
  enable it to prove (and enforce) more properties.

  From Dependent Types, Liquid Types takes the idea of refinement types further
  and enable automatic inference of types thus reducing the need for manual
  type annotations.  Lastly, LIQUIDHASKELL fixed the soundness of Liquid Types
  under lazy evaluation while retains its advantage of automatic type
  inference.

  % This is from the template
  Now that the content of the papers has been explained in more detail,
  this section should
  \begin{itemize}
    \item put the papers into context (as seen in the class). E.g. how do the
    ideas presented in the paper related to ideas seen in class; are the techniques
    an extension; if they are completely different, explain how (and perhaps why), etc.

    \item explain the connections and/or differences between the papers.
      E.g. if they present different approaches, how do these compare, what are the pros/cons?
      If the techniques build on each other, how are they connected?
      If they are part of a larger system, how do they complement each other?
      Etc.

  \end{itemize}

\section{Conclusion}

  What is the 'take-home-message'?

%%
%% Bibliography
%%

%% Either use bibtex (recommended),

\bibliography{main}

%% .. or use the thebibliography environment explicitely



\end{document}
