\documentclass[a4paper,UKenglish]{lipics-v2016}
% This template is based on the template for producing LIPIcs articles.
% See lipics-manual.pdf for further information.
% https://www.dagstuhl.de/en/publications/lipics/instructions-for-authors/
% for section-numbered lemmas etc., use "numberwithinsect"

% we do not need the copyright line or the DOI
\renewcommand{\copyrightline}{}
\DOIPrefix{}

\bibliographystyle{plainurl}% the recommended bibstyle

% Author macros::begin %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Advanced Type-Based Analysis: Final Report}

%% Add all author names here
\author[1]{Nikita Ziuzin}
\author[2]{Joachim Bard}
\author[3]{Hizbullah Abdul Aziz}
% List email addresses in the same order as the authors
\affil[1]{\texttt{nzyuzin93@gmail.com}}
\affil[2]{\texttt{s9jobard@stud.uni-saarland.de}}
\affil[3]{\texttt{s8hijabb@stud.uni-saarland.de}}

\begin{document}

\maketitle


\section{Introduction}
\label{sec:intro}

  Many programming languages have some sort of type system that enforces a
  certain notion of correctness from a program.  For example, a function that
  adds two numbers, when defined correctly, should only works on numbers and
  nothing else (which are reflected on the type of its arguments).  A
  typechecker ensures that a program is well-typed, that is, everything has a
  well-defined type, by statically checking the type of every part of a program
  (usually) during compile time.  In this particular example, trying to call
  the addition function on a number and a boolean valua should make the
  typechecker rejects the program as not well-typed.

  However, well-typed programs can still go wrong.  Standard type systems found
  in mainstream programming language such as C, Java, or even the more advanced
  ones in ML or Haskell can't prevent some bad runtime behaviors such as
  division by zero.  Thus there have been some research on extending types and
  type systems to become more expressive.  In particular, there is a notion of
  Dependent Types, which are types that depend on some values.  Starting with
  statically checking array bound as a motivating example, Dependent Types
  have spawned many subsequent researches, two of which will be presented in
  this report.

  In this summary, we will begin with the high-level summary Dependent Types
  itself~\cite{Xi:1998:EAB}.  We will then proceed with a brief explanation of
  Liquid Types~\cite{Rondon:2008:LT}, which is types \emph{refined} with
  predicates that builds on the ideas of Dependent Types.  In particular,
  Liquid Types make it possible to reduce the number of annotation needed while
  at the same time retains the benefit of Dependent Types.  Finally, we
  summarizes the approach taken to bring Liquid Types under lazy evaluation
  settings~\cite{Vazou:ICFP:2014}.

  % This is from of the template
  In high-level terms
  \begin{itemize}
    \item introduce the topic

    \item explain why it is relevant/related to the course (i.e. how is it related
    to static analysis)

    \item explain why the topic is important (e.g. where and how can it be used)

    \item introduce your chosen papers and briefly explain how they are related
    to the topic
  \end{itemize}

\section{Summary of Papers}

\section{Paper 1: Eliminating array bound checking through dependent types}

\abstract

Originating in type theory, dependent types is a powerful concept for extending
type checking for the purposes of program analysis. Its applications allow to
perform many advanced forms of analysis with a little work on the programmer's
side (providing correct type annotations). One of such applications is
elimination of array bound checking during the compilation time using dependent
types.

\subsection{Note on dependent types}

The concept of the dependent types is quite rich and well-studied, the main
idea behind it is that dependent type systems unlike systems with simple types
allow for types to depend on or vary with values. Examples to simple types are
products, functions or atomic types that are \texttt{int} or \texttt{bool}.

A classical example of a dependent type is a type of lists $List_\gamma(N)$ of
a given length $N : \mathbb{N}$ over some type $\gamma$. The elements of such
type are $Nil_\gamma : List(0)$ and $Cons_\gamma(x, l) : List(N+1)$, where $x :
\gamma$ and $l : List_\gamma(N)$. Now let's consider a function that takes $n :
\mathbb{N}$ and returns a list of length $n$ containing $0$ in all entries.
This function would have type $\Pi n : \mathbb{N}.List_\mathbb{N}(n)$, a type
of functions for which the type of the return value depends on the argument.
The important point of the dependent typing is that it reveals more information
about the function.

In the following no knowledge of the full theory of dependent types is assumed,
and furthermore, no such knowledge is required, as for the purpose of the paper
only a limited part of the dependent type systems is used. This part is fully
explained in the following sections. The limitation excludes terms of general
form (that possibly make type checking undecidable) from type formers and only
allows for a limited set of decidable refinements to be used.

However, some familiarity with the type theory notation and/or ML syntax
will be assumed.

For a full study on the dependent types the reader is referred to the following
paper \cite{Hofmann97syntaxand}.

\subsection{Introduction}
The problem of an absence of runtime array bound checking in lower level
languages, such as C has led to numerous problems over time. However, even in
the languages that perform this runtime checking, it still makes an issue
imposing additional execution time costs for every array access.  Moreover,
this issue is present not only for the imperative languages with array
accesses, but also for more general cases of accessing list elements through
pattern matching in strongly typed functional languages. Traditional compiler
optimizations for this analysis do not guarantee that they are always able to
infer the correct array bounds \cite{Gupta:1993}\cite{Suzuki:1977}.

On the other hand, approach that relies on the dependent types makes array
bounds checking a part of the type checking, with appropriate (albeit minor)
help from the programmer in form of more advanced type annotations. These
annotations construct a set of linear arithmetic value constraints which can be
efficiently checked for the satisfiability.

To give a practical validation to the  concept all examples are given in a
language that implements ML fragment extended with a limited form of dependent
types. This fragment matches the core language of ML standard, making omission
for exceptions and modules, which are not relevant to the issue by themselves,
and contains the addition of type annotations that cover the required
expressibility properties.

In the following, a value on which a dependent type is formed will be called an
\emph{index object}. For example, a type of integer lists in ML would be
written as follows: \texttt{int list}. To express a type of the integer lists
of length two one writes: \texttt{int list(2)}, where 2 is exactly the
\emph{index object}.  Then a function to append two integer lists of fixed
lengths \texttt{n} and \texttt{m} would have the type: \texttt{int(n) -> int(m)
-> int(n+m)}.

It is worth notice that if arbitrary terms can be used as index objects, then
type checking becomes undecidable, since it would involve arbitrary
computations.  Considering this, severe limitations on the index objects will
be imposed, distinguishing them from the general term of the implemented
language.

\subsection{Types and typing}

\subsubsection{The language of types}
This section presents a syntax for writing the index objects and forming the
type annotations in the implemented language.

Type indices may be integer or boolean expressions defined below.
$a$ ranges over the index variables.
\begin{align*}
  \text{Integer index } & i,j &&::= &&& a|i+j|i-j|i*j|div(i,j)\\
                &     &&    &&& |min(i,j)|max(i,j)\\
                &     &&    &&& |abs(i)|sign(i,j)|mod(i,j)\\
  \text{Boolean index } & b &&::= &&& a|false|true\\
                &     &&    &&& |i < j | i \leq j \\
                &     &&    &&& |i = j|i \geq j | i > j\\
                &     &&    &&& |\neg b| b_1 \land b_2 | b_1 \lor b_2\\
  \text{Index } & d &&::= &&& i|b
\end{align*}

$\delta$ ranges over base types or \emph{basic type families}, that may be built-in
(\texttt{int}, \texttt{array}) or user-declared. $\alpha$ stands for type variables.
\begin{align*}
  \text{Index sort } & \gamma &&::= &&& int | bool | \{ a: \gamma | b \}\\
  \text{Types } & \tau &&::= &&& \alpha | (\tau_1, \dots, \tau_n)\delta (d_1, \dots, d_k) | \{ a: \gamma | b \}\\
                &      &&    &&& | \tau_1 * \dots * \tau_n | \tau_1 \to \tau_2\\
                &      &&    &&& | \Pi a: \gamma. \tau | \Sigma a: \gamma. \tau
\end{align*}

For clarity, the type constructor with no arguments or indices is written
simply as $\delta$ (instead of $()\delta()$).

Notation $\{ a: \gamma | b \}$ denotes those elements of $\gamma$ that satisfy
the boolean constraints $b$.  $nat$ serves as an abbreviation for $\{ a: int |
a \geq 0 \}$. The types $\Pi a: \gamma. \tau$ and $\Sigma a: \gamma. \tau$
denote universal and existential quantification of $a$ respectively.

In the examples, implementation language uses uses \texttt{\{a:g\} t} for $\Pi
a: \gamma. \tau$ and \texttt{[a:g] t} for $\Sigma a: \gamma. \tau$. One can
also write \texttt{\{a:g | b\} t} for $\Pi a: \{a: \gamma | b\}. \tau$.

\subsubsection{Built-in type families}

The examined ML fragment extension includes type families for integers, arrays
and booleans as built-in.

\begin{itemize}
  \item For every integer $n$, \texttt{int(n)} is a singleton type containing
    only $n$.
  \item \texttt{bool(true)} and \texttt{bool(false)} are singleton types for
    \texttt{true} and \texttt{false} respectively.
  \item For a natural number $n$, \texttt{'a array(n)} is the type for arrays
    of size $n$ over a type \texttt{'a}

    One can also omit indices in type specification, in that case they will
    interpreted existentially. For instance, the type \texttt{int array} will
    be converted to $\Sigma n : nat$.\texttt{int array}$(n)$, which can be read
    as an array of an unknown size $n$.
\end{itemize}

\subsubsection{Datatype refinements}

As in ML standard, besides having such types as \texttt{int}, \texttt{bool} and
\texttt{array}, users can declare their own types with the usual construct.
Consider a declaration of a list type:
\begin{verbatim}
datatype 'a list =
  nil
| :: of 'a * 'a list
\end{verbatim}

There's then a possibility to refine this declaration with type indices:
\begin{verbatim}
typeref 'a list of nat
with nil <| 'a list(0)
| :: <| {n:nat} 'a * 'a list(n) -> 'a list(n+1)
\end{verbatim}

And this refinement forms a dependent type for lists with the list size exposed
in the type, that was discussed earlier.

\subsubsection{Example functions}

\begin{lstlisting}[caption={The dot product function},label=dotprod,captionpos=t,float,abovecaptionskip=-\medskipamount]
assert length <| {n:nat} 'a array(n) -> int(n)
and sub <| {n:nat} {i:nat | i < n} 'a array(n) * int(i) -> 'a

fun dotprod(v1, v2) =
  let
    fun loop(i, n, sum) =
      if i = n then sum
      else loop(i+1, n, sum + sub(v1, i) * sub(v2, i))
    where loop <| {n:nat} {i:nat | i <= n}
      int(i) * int(n) * int -> int
  in
    loop(0, length v1, 0)
  end
where dotprod <| {p:nat} {q:nat | p <= q }
  int array(p) * int array(q) -> int
\end{lstlisting}

To introduce the language, Figure \ref{dotprod} shows an example of a function
calculating the dot product of two vectors organized as arrays.

The example illustrates the use of the dependent types in the language, where
dependent types are given after the \texttt{where} keyword. It's worth notice
that if one decides to omit the lines containing \texttt{where} from the
function code, the function can be accepted by any compiler adhering to the ML
standard. Predefined function \texttt{length} returns the length of a given
array and \texttt{sub} returns an element at the specified position. Note that
the type declaration of \texttt{sub} ensures that the array access is always
within the array bounds and therefore avoids the need for the runtime bound
checking. The same use of the dependent types is applied to \texttt{loop} and
\texttt{dotprod} type declarations.

\begin{lstlisting}[caption={The reverse function for lists},label=reverse,captionpos=t,float,abovecaptionskip=-\medskipamount]
fun reverse(l) =
let
  fun rev(nil, ys) = ys
    | rev(x::xs, ys) = rev(xs, x::ys)
  where rev <| {m:nat} {n:nat}
    'a list(m) * 'a list(n) -> 'a list(m+n)
in
  rev(l, nil)
end
where reverse <| {n:nat} 'a list(n) -> 'a list(n)
\end{lstlisting}

Another example on Figure \ref{reverse} illustrates a function computing the
reverse of a given list. The type here ensures that the size of the list
doesn't change from the argument to the result.

\begin{lstlisting}[caption={The filter function for lists},label=filter,captionpos=t,float,abovecaptionskip=-\medskipamount]
fun filter p nil = nil
  | filter p (x::xs) =
    if p(x) then x::(filter p xs)
    else filter p xs
where filter <| {m: nat}
  ('a -> bool) -> 'a list(m) -> [n:nat | n <= m] 'a list(n)
\end{lstlisting}

Figure \ref{filter} depicts a function that makes use of existentially
quantified dependent types. This function removes all elements from the list
$l$ which do not satisfy the predicate $p$. Obviously, for an arbitrary $p$,
it's impossible to give an exact value of the resulting list size without
computing the function. Therefore this value cannot be accepted as a function
argument, but still can have a specific bound, that is -- the size of the
resulting list should not be greater than the size of the initial list.
\texttt{[n:nat | n <= m] 'a list(n)} expresses exactly this, yielding a list of
some definitive size, which is bound by a fixed number.

More details to the language and example programs can be found in
\cite{Xi:1998}\cite{XiPhd:1998}.

\subsection{Constraints}

The given definition of dependent types leads to a formation of constraints on
the values used in the program, where constraints are boolean index expressions
enriched with explicit quantifiers and implication.

The language for constrains is as follows:
\begin{align*}
  \text{Constraints } &\phi &&::= &&&b | \phi_1 \land \phi_2 | b \supset \phi\\
                      &     &&    &&&| \exists a : \gamma. \phi | \forall a : \gamma. \phi
\end{align*}

Constraints are extracted from the program after it has been type checked
ignoring dependent types and verified correct by the ML type inference.

\subsubsection{Constraint generation}

Consider the \texttt{rev} function from the Figure \ref{reverse}:
\begin{verbatim}
fun rev(nil, ys) = ys
  | rev(x::xs, ys) = rev(xs, x::ys)
where rev <| {m:nat} {n:nat}
  'a list(m) * 'a list(n) -> 'a list(m+n)
\end{verbatim}

For the first case of \texttt{rev}, namely \texttt{rev(nil, ys) = ys} the two
variables $M$ and $N$ are generated according to the index objects of the
function type.  Then \texttt{nil} is checked against the type \texttt{'a
list($M$)} and \texttt{ys} is checked against the type \texttt{'a list($N$)}.

This generates constraints $M = 0$ and $N = \texttt{n}$, where \texttt{ys} is
assumed to be of type \texttt{'a list(n)}. Afterwards, the type of the right
hand side of the pattern matching clause, \texttt{ys}, is checked against
\texttt{'a list($M$ + $N$)}, the result type for \texttt{rev}. This yields a
constraint $M + N = \texttt{n}$.

Thus, the first clause of the pattern matching in the function definition
generates the constraint:

$\forall n : nat. \exists M : nat. \exists N : nat. (M = 0 \land N = n \supset M + N = n)$

This is simplified by the elimination of the existential variables to:

$\forall n : nat. (0 + n = n)$

which is stored and easily verified on the later stages.

Applying the same procedure to the second case in the definition,
\texttt{rev(x::xs, ys) = rev(xs, x::ys)} provides the following constraint:

$\forall m : nat. \forall n : nat.(m + 1) + n = m + (n + 1)$,

that can also be satisfied.

\subsubsection{Constraint solving}

The reader may notice, that constraints obtained by the given syntactic rules
generate a set of linear integer arithmetic equations and thus can be solved by
any LIA solver. For the purpose, many various methods exists, such as
Fourirer-Motzkin, Simplex, SUP-INF, etc.

Applying these methods to the set of constrains will ensure that type checking
on dependent types passes if and only if the constrains are satisfiable by the
solving procedure.

\subsubsection{Experiments}

The computational experiments with the programs written using this bound
checking elimination technique show a stable performance gain (10-70\%) among
the studied programs. The associated code changes (type annotations) are
considerably insignificant in size compared to the overall size of the program.

The exact implementation and experimental details are found in \cite{Xi:1998}.

\subsection{Related work}

The original paper became the influential example of a practical dependent
types application for the purposes of static analysis. The ideas got
implemented in form of DML dialect of ML \cite{XiPhd:1998} and inspired further
developments in refinement types.


\subsection{Paper 2: Liquid Types}

\subsubsection{Goal/Problem}
The approach presented in~\cite{Xi:1998:EAB} needs many manual type annotations.
This is tedious and sometimes it is not easy to come up with the correct dependent type.
Therefore Rondon et al. introduced logically qualified data types or short liquid types~\cite{Rondon:2008:LT}.
The advantage of liquid types is that those types can be infered automatically.

\subsubsection{Main Idea}

Liquid types are dependent types where we restrict the refinements to conjunctions of predicates (qualifiers) and their negations.
This restriction is similar to predicate abstraction.
But unlike for predicate abstraction the set of predicates are chosen manually and not by examining the program code.
This choice depends on the property one wants to show by the analysis.
For example one can express types like $i:: \{\nu:$ \lstinline{int} $\mid 0 \le \nu \land \nu <$ \lstinline{len} $a\}$.
This type ensures that accessing array $a$ at index $i$ is safe.
Note that we used a predicate which compares the special value variable $\nu$ with the length of the array $a$.
To infer such types automatically 3 steps are executed:
\begin{enumerate}
    \item Step 1 performs a classical type inference as seen in class.
        The obtained types are transformed into templates where the unknown type refinements are expressed by so called liquid type variables.
    \item Step 2 generates a set of subtype constraints which restrict choices for the liquid type variables.
        This step is guided by the syntax.
    \item Step 3 tries to solve these constraints by embedding them into a decidable logic.
\end{enumerate}

%mention this when using the notation
%A base type $B$ is a short form of $\{\nu: B \mid true\}$.
%We write $\kappa$ for $\{\nu: B \mid \kappa\}$ with a liquid type variable $\kappa$ if the base type $B$ is clear from the context.

\begin{lstlisting}[caption={Example Program},label=lst:exmpl,captionpos=t,float,abovecaptionskip=-\medskipamount]
let max x y =
    if x > y then x else y
\end{lstlisting}

To illustrate those 3 steps we look at an example in listing~\ref{lst:exmpl}.

%path sensitive
In step 1 classical type inference yields the type $x:$ \lstinline{int} $\rightarrow y:$ \lstinline{int} $\rightarrow$ \lstinline{int} for \lstinline{max}.
Using this type we build the template $x: \kappa_x \rightarrow y: \kappa_y \rightarrow \kappa$.
The liquid type variables $\kappa_x, \kappa_y, \kappa$ are the unknown refinements for the arguments $x$ and $y$ plus the body of \lstinline{max}.
Step 2 generates the following constraints since the body is an \lstinline{if}-expression:
\begin{align}
    \label{algn:max}
    x: \kappa_x; y: \kappa_y; (x > y) \vdash \{\nu: \texttt{int} \mid \nu = x \} <: \kappa\\
    x: \kappa_x; y: \kappa_y; \neg(x > y) \vdash \{\nu: \texttt{int} \mid \nu = y \} <: \kappa
\end{align}
The first constraint forces the type of the \lstinline{then}-branch (namely $\{\nu: \texttt{int} \mid \nu = x \}$) to be a subtype
of the body ($\kappa$) in the environment where $x$ and $y$ have the appropriate types as well as the branch condition is true.
The second contraint is analogous.
Note that in the environment the branch condition is false.
So path sensitivity is introduced by adding assumptions to the environment.
In step 3 we set $\kappa_x$ and $\kappa_y$ to true because \lstinline{max} can be called by any integer.
Moreover we use a theorem prover to get the strongest conjunction of qualifiers in $\mathbb{Q}^\star$ for $\kappa$ which staisfies the two constraints.
We get $x \le \nu \land y \le \nu$ since if $x > y \land \nu = x$ then $x \le \nu \land y \le \nu$ and if $\neg(x > y) \land \nu = y$
then $x \le \nu \land y \le \nu$.
Thus we obtain the type:
\begin{center}
    \lstinline{max} :: $x:$ \lstinline{int} $\rightarrow y:$ \lstinline{int} $\rightarrow \{\nu:$ \lstinline{int} $\mid x \le \nu \land y \le \nu\}$.
\end{center}

%recursion
%higher-order functions
%polymorphism and array bounds checking

\subsubsection{Details}

The liquid type checking is formalized in a $\lambda$-calculus with \lstinline{if}- and \lstinline{let}-expressions, recursive funtions and polymorphism.
Intuitively a liquid type $T$ is well-formed under the environment $\Gamma$ ($\Gamma \vdash T$) if the free variables occuring in the refinements of $T$ are bound in $\Gamma$.
The environment $\Gamma$ consists of type bindings ($x: T$) and predicates.
For example we can say that $\{\nu:$ \lstinline{int} $\mid x \le \nu \land y \le \nu$ is well-formed under $x:$ \lstinline{int}; $y:$ \lstinline{int}.
In addition there is also a decidable subtype judgment $\Gamma \vdash T_1 <: T2$ stating that $T_1$ is a subtype of $T_2$ under environment $\Gamma$.
As example consider $x:$ \lstinline{int}; $y:$ \lstinline{int}; $(x > y) \vdash \{\nu = x \} <: x \le \nu \land y \le \nu$
The liquid type judgement $\Gamma \vdash_{\mathbb{Q}} e: T$ says that the expression $e$ has type $T$ under environment $\Gamma$ if one considers the logical qualifiers $\mathbb{Q}$.
For example we can show $\Gamma \vdash_{\mathbb{Q}}$ \lstinline{let} $s$ \lstinline{= sum (k - 1) in s + k} $: T$ if $T$ is well-formed under $\Gamma$ and the refinements of $T$ consists of conjunctions over logical qualifiers in $\mathbb{Q}^\star$, plus
$\Gamma \vdash_{\mathbb{Q}}$ \lstinline{sum (k - 1)} $:T_1$ and $\Gamma; x: T_1 \vdash_{\mathbb{Q}}$ \lstinline{s + k} $: T$.
Soundness of this liquid type checking is proven in two steps.
Firstly, one defines an exact type system $\Gamma \vdash e: T$ using an undecidable subtype judgment.
Then soundness is obtained using the following theorems, where $\succ$ is the single evaluation step relation:
\begin{enumerate}
    \label{thm:sound}
    \item (Overapproximation) If $\Gamma \vdash_{\mathbb{Q}} e: T$ then $\Gamma \vdash e: T$.
    \item (Preservation) If $\Gamma \vdash e: T$ and $e \succ e'$ then $\Gamma \vdash e': T$.
    \item (Progress) If $\emptyset \vdash e: T$ and $e$ is not a value then $\exists e'. e \succ e'$.
\end{enumerate}

The 3 steps of the type inference can be implemented in the formal setting.
%The examples given in Subsection~\ref{subsec:exmpl} illustrate some cases of this algorithm.
Also for this algorithm called \lstinline{Infer}($\Gamma, e, \mathbb{Q}$) one can show some theorems:
\begin{enumerate}
    \item \lstinline{Infer}($\Gamma, e, \mathbb{Q}$) terminates.
    \item If \lstinline{Infer}$(\Gamma, e, \mathbb{Q} = T$ then $\Gamma \vdash_{\mathbb{Q}} e: T$.
    \item If \lstinline{Infer}($\Gamma, e, \mathbb{Q}$) returns no type then $\nexists T.\, \Gamma \vdash_{\mathbb{Q}} e: T$.
\end{enumerate}

Combining these with the theorems~\ref{thm:sound} we obtain that if \lstinline{Infer}$(\emptyset, e, \mathbb{Q} = T$ then the evalution of $e$ won't get stuck.

\subsubsection{Evaluation}

In order to assess the advantage of this approach liquid type inference is implemented in \textbf{DSolve}.
It is tested over a set of benchmarks from the \textbf{DML} project~\cite{Xi:1998:EAB}.
To allow array bounds checking the qualifiers are of the form $\nu \bowtie X$ where $\bowtie \in \{<, \le, =, \neq, \geq, >\}$ and $X \in \{0, \star,$ \lstinline{len} $\star$\}.
In most cases \textbf{DSolve} can prove the safety of array accesses without additional annotations i.e. qualifiers.
If qualifiers are missing the infered types help to identify which ones should be added.
Compared to the \textbf{DML} project the impact is very big.
Whereas in the \textbf{DML} project 17\% of all lines (31\% of characters) are annotations \textbf{DSolve} only needs 1\% of lines and characters.

  \subsection{Refinement Types For Haskell}

    \subsubsection{Lazy Evaluation}

      Some programming languages, most notably Haskell, use a different
      evaluation strategy called lazy evaluation.  Intuitively, lazy evaluation
      means that computations is only performed if the results of those
      computations are needed.  For example, if a program specifies that the
      result of \texttt{div 10 5} be printed on screen, then the this
      computation i.e. divide 10 by 5 will be computed.  Conversely, if the
      result of \texttt{div 10 5} is never needed, then it will never return a
      value.  

      It turns out that classical approach on refinement types is unsound under
      lazy evaluation, in the sense that the typechecker will admit some bad
      behaviours.  Consider the following example.
      \begin{verbatim}
        type Pos = { v:Int | v >  0 }
        type Nat = { v:Int | v >= 0 }

        div :: n:Nat -> d:Pos -> { v:Nat | v < n } 
        (+) :: x:Int -> y:Int -> { v:Int | v = x + y }
        diverge :: Int -> { v:Nat | false } 

        explode :: Int -> Int
        explode x = let n = diverge 1 in 
                    let y = 0 in 
                    div x y
      \end{verbatim}

      When checking for the type of \texttt{y}, the typechecker will generate
      the constraint
      $$\mathtt{false}\wedge(y=0)\implies(v=0)\implies(v>0)$$
      which any SMT solver will accept since we have falsity in the antecedent.
      Note that the \texttt{false} part in the constraint comes from
      \texttt{diverge}, which essentially states that this function diverges
      i.e. will not return any value.

      This constraint is of course sound under eager evaluation, since we will
      never get into calling \texttt{div} with the second argument equals to
      zero i.e. division by zero.  Under lazy evaluation however, since
      \texttt{n} is not needed in the computation of the result of
      \texttt{explode}, the computation will jump straight into evaluating
      \texttt{div} thus resulting in division by zero.  Recall that the type of
      \texttt{div} should prevent this since it requires its second argument to
      have type \texttt{Pos}, a positive integer, which proves the unsoundness
      of the previous approach.

    \subsubsection{Solution}
      
      To restore soundness, the main idea is to label binders as potentially
      diverging and omit those in the typing constraint.  Returning to the
      previous example, by omitting \texttt{diverge} in the constraint, we have
      $$\mathtt{true}\wedge(y=0)\implies(v=0)\implies(v>0)$$
      which is now an invalid constraint and thus the typechecker will reject
      this as not well-typed.

      One way to implement this is to have a \emph{stratified} type system
      consisting of types which may diverge and those which may not.
      \begin{itemize}
        \item Initially, every binders have types which may diverge.  This is
          sound but very imprecise since now there is no constraint at all.
        \item Then termination analysis is used to determine which binders
          actually terminate, thus getting some constraints back.
      \end{itemize}

      Now it should be clear that the accuracy of this approach depends on the
      termination analysis.  Loosely speaking, there are three main points of
      it.
      \begin{itemize}
        \item As with previous approaches, the refinements itself must be
          restricted into some sort of decidable logic, because otherwise
          typechecking would be undecidable.  Here refinements are drawn from
          QF-EUFLIA, the decidable logic of equality, uninterpreted
          functions and linear arithmetic.
        \item To prove termination of inductively defined functions i.e.
          recursive functions, it employs some default size measures and check
          whether those measures decrease on each (recursive) function call.
          This approach is of course not unlike one found in other programming
          languages such as Coq. Using these default measures, termination can
          be proven automatically.
        \item If the automatic termination analysis fails, then the size
          measure must be explicitly specified by the user.
      \end{itemize}

    \subsection{Implementation and Result}
      
      LIQUIDHASKELL, a Haskell extension with refinement types succesfully
      brings refinement typechecking under lazy evaluation.  The termination
      analysis used in LIQUIDHASKELL is usable since it only requires about 1
      line of code of explicit size measure per 100 line of code, thus
      retaining the advantage of previous approach while maintaining soundness
      under lazy evaluation.

\section{Context and Connection (feel free to change the section names)}

  As briefly mentioned in section~\ref{sec:intro}, Dependent Types and its
  subsequent improvements are extensions to the standard type systems discussed
  in the class.  These extensions make the type system more expressive and thus
  enable it to prove (and enforce) more properties.

  From Dependent Types, Liquid Types takes the idea of refinement types further
  and enable automatic inference of types thus reducing the need for manual
  type annotations.  Lastly, LIQUIDHASKELL fixed the soundness of Liquid Types
  under lazy evaluation while retains its advantage of automatic type
  inference.

  % This is from the template
  Now that the content of the papers has been explained in more detail,
  this section should
  \begin{itemize}
    \item put the papers into context (as seen in the class). E.g. how do the
    ideas presented in the paper related to ideas seen in class; are the techniques
    an extension; if they are completely different, explain how (and perhaps why), etc.

    \item explain the connections and/or differences between the papers.
      E.g. if they present different approaches, how do these compare, what are the pros/cons?
      If the techniques build on each other, how are they connected?
      If they are part of a larger system, how do they complement each other?
      Etc.

  \end{itemize}

\section{Conclusion}

  What is the 'take-home-message'?

%%
%% Bibliography
%%

%% Either use bibtex (recommended),

\bibliography{main}

%% .. or use the thebibliography environment explicitely



\end{document}
