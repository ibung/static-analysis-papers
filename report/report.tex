\documentclass[a4paper,UKenglish]{lipics-v2016}
% This template is based on the template for producing LIPIcs articles.
% See lipics-manual.pdf for further information.
% https://www.dagstuhl.de/en/publications/lipics/instructions-for-authors/
% for section-numbered lemmas etc., use "numberwithinsect"

% we do not need the copyright line or the DOI
\renewcommand{\copyrightline}{}
\DOIPrefix{}

\bibliographystyle{plainurl}% the recommended bibstyle

% Author macros::begin %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Advanced Type-Based Analysis: Final Report}

%% Add all author names here
\author[1]{Nikita Ziuzin}
\author[2]{Joachim Bard}
\author[3]{Hizbullah Abdul Aziz}
% List email addresses in the same order as the authors
\affil[1]{\texttt{nzyuzin93@gmail.com}}
\affil[2]{\texttt{s9jobard@stud.uni-saarland.de}}
\affil[3]{\texttt{s8hijabb@stud.uni-saarland.de}}

\begin{document}

\maketitle


\section{Introduction}

Programming languages employ type systems to enforce a certain notion of a
program correctness. For example, a function $+$ that adds two numbers when
defined correctly should only be applied to numbers and nothing else (which
is reflected by the types of the arguments). A typechecker ensures that a
program is well-typed, that is, everything has a well-defined type, by
statically checking the type of every program part during the compilation
time.  When one tries to call $+$ on a number and a boolean, the typechecker
rejects the program as not well-typed.

However, well-typed programs can still go wrong. Standard type systems found
in mainstream programming languages such as \textbf{C}, \textbf{Java}, or
even the more advanced ones in \textbf{ML} or \textbf{Haskell} can't prevent
some bad runtime behaviors such as division by zero. The idea of extending
types and type systems to become more expressive has been well studied. In
particular, there is a notion of dependent types, which are the types that
depend on some values. Starting with statically checking array bound
satisfaction as a motivating example, dependent types have spawned numerous
subsequent developments, two of which are presented in this report.

In this report, we begin with a high-level notion of dependent types, their
restrictions, and their application to eliminate array bound checking during
runtime \cite{Xi:1998:EAB}. Then we proceed with a brief explanation of Liquid
Types~\cite{Rondon:2008:LT}, which is types \emph{refined} with predicates that
build on the idea of dependent types. In particular, Liquid Types make it
possible to reduce the number of type annotations needed and also retain the
benefit of dependent types.  Finally, we summarize a way to bring Liquid Types
under lazy evaluation settings~\cite{Vazou:ICFP:2014}.

\section{Papers}

\subsection{Paper 1: Eliminating array bound checking through dependent types}

The problem of array bound checking is persistent in many programming
languages. That is, the languages that are said to be memory-safe in the
runtime such as \textbf{ML} or \textbf{Java} need to include the checks for
array access in the compiled code resulting in the increased execution time
costs when the checking is redundant. On the other hand, the languages that do
not employ array bound checking in runtime, such as \textbf{C} result in
particularly unpleasant runtime bugs when the programmer is not careful.

To address this issue, several methods have been developed over time. Many of
such methods try to eliminate redundant checks without any help from the
programmer, through generation of loop invariants which is not generally
decidable in theory and may be difficult in practice.

A possible approach to solving this problem is to use dependent types. This
way, array bound checking is performed during the type checking through an
integer constraint validation. Naturally, that brings no additional cost for
the runtime.  However, this approach requires some help from the programmer,
namely, correct type annotations for the program. The following sections will
concern the implementation and application for the idea.

\subsubsection{Dependent types}

The concept of the dependent types is quite rich and well-studied, the main
idea behind it is that dependent type systems unlike systems with simple types
allow for types to depend on or vary with values. Examples to simple types are
products, functions or atomic types that are \texttt{int} or \texttt{bool}.

A classical example of a dependent type is a type of lists $List_\gamma(N)$ of
a given length $N : \mathbb{N}$ over some type $\gamma$. The elements of such
type are $Nil_\gamma : List(0)$ and $Cons_\gamma(x, l) : List(N+1)$, where $x :
\gamma$ and $l : List_\gamma(N)$. Now let's consider a function that takes $n :
\mathbb{N}$ and returns a list of length $n$ containing $0$ in all entries.
This function would have type $\Pi n : \mathbb{N}.List_\mathbb{N}(n)$, a type
of functions for which the type of the return value depends on the argument.
The important point of the dependent typing is that it reveals more information
about the function.

It's worth notice that the general definition of the dependent types allows for
terms of any kind to be used in the typing expression. Naturally, this makes
type checking equivalent to an execution of an arbitrary program, which in
turns makes it undecidable in general.

To avoid this problem a restricted form of dependent types is required that
will allow to remain decidable and at the same time be expressive enough to
solve the problem at hand.

For a full study on the dependent types the reader is referred to
\cite{Hofmann97syntaxand}.

\subsubsection{Restriction on dependent types}

\textbf{DML}, a language that covers an \textbf{ML} fragment extended with a
limited form of dependent types is used to give a practical validation for the
solution. This fragment matches the core language of the \textbf{ML} standard,
making omission for exceptions and modules, and contains the addition of the
restricted form for the dependent type annotations.

This form is consists of linear integer arithmetic (LIA) and boolean
expressions that can be solved by a LIA solver.

In the concrete syntax a value on which a dependent type is formed will be
called an \emph{index object}. For example, a type of integer lists in
\textbf{ML} would be written as follows: \texttt{int list}. To express a type
of the integer lists of length two one writes: \texttt{int list(2)}, where 2 is
exactly the \emph{index object}. Then a function to append two integer lists of
some fixed lengths \texttt{n} and \texttt{m} would have the type:
\texttt{int(n) -> int(m) -> int(n+m)}.

Index objects can be boolean constants $false$, $true$ or boolean expressions
composed by $\land$, $\lor$, $\neg$, and integer expressions resulting in
either boolean or integer that are composed with the usual operations on
integers: $+$, $-$, $*$, $div$, $min$, $max$, $abs$, $sign$, $mod$, $>$, $<$,
$\ge$, $\le$, $=$.

To quantify the variables used in the index object one writes \texttt{\{a:g\}
t} for the universal quantification (often written in the literature as $\Pi a:
\gamma.  \tau$) and \texttt{[a:g] t} for the existential quantification
($\Sigma a: \gamma. \tau$).  $\{ a: g | b \}$ corresponds to those elements of
a type $g$ that satisfy the boolean constraints $b$. There's also a shorthand
\texttt{\{a:g | b\} t} for $\Pi a: \{a: \gamma | b\}. \tau$. And then $nat$
serves as an abbreviation for \texttt{\{a: int | a >= 0\}}.

\begin{lstlisting}[caption={The filter function for lists},label=filter,captionpos=t,float,abovecaptionskip=-\medskipamount]
fun filter p nil = nil
  | filter p (x::xs) =
    if p(x) then x::(filter p xs)
    else filter p xs
where filter <| {m: nat}
  ('a -> bool) -> 'a list(m) -> [n:nat | n <= m] 'a list(n)
\end{lstlisting}

Figure \ref{filter} serves as an example program. It depicts a function that
given a list over some type $A$ and a predicate on $A$ returns a list
containing all the elements that satisfy the predicate. The dependent types
annotation, \texttt{where filter <| \{m: nat\} ('a -> bool) -> 'a list(m) ->
[n:nat | n <= m] 'a list(n)} displays the relation between input and output
lists, that is that the resulting list is shorter than the initial.

\begin{lstlisting}[caption={The reverse function for lists},label=reverse,captionpos=t,float,abovecaptionskip=-\medskipamount]
fun reverse(l) =
let
  fun rev(nil, ys) = ys
    | rev(x::xs, ys) = rev(xs, x::ys)
  where rev <| {m:nat} {n:nat}
    'a list(m) * 'a list(n) -> 'a list(m+n)
in
  rev(l, nil)
end
where reverse <| {n:nat} 'a list(n) -> 'a list(n)
\end{lstlisting}

For another example consider the function that returns a reverse for a given
list. Figure \ref{reverse} displays the implementation of such a function in
\textbf{DML}. The type here ensures that the size of the list doesn't change
from the argument to the result.

\subsubsection{Type checking on the restriction}

The first step of type checking in \textbf{DML} is to ignore index objects and
ensure that the program type checks without them as an \textbf{ML} program.
Afterwards, index objects are checked. Since the allowed expressions form
linear integer inequalities, constraints on the integers can be extracted form
the index objects and checked separately.

Consider for the example the \texttt{rev} function from the Figure
\ref{reverse}:
\begin{verbatim}
fun rev(nil, ys) = ys
  | rev(x::xs, ys) = rev(xs, x::ys)
where rev <| {m:nat} {n:nat}
  'a list(m) * 'a list(n) -> 'a list(m+n)
\end{verbatim}

It uses pattern matching as a function definition and thus one can consider the
two cases of \texttt{rev} separately. For the first case, \texttt{rev(nil, ys)
= ys} the two variables $M$ and $N$ are generated according to the index
objects of the function type. Then \texttt{nil} is checked against the type
\texttt{'a list($M$)} and \texttt{ys} is checked against the type \texttt{'a
list($N$)}.

This generates the constraints $M = 0$ and $N = \texttt{n}$, where \texttt{ys}
is assumed to be of type \texttt{'a list(n)}. Afterwards, the type of the right
hand side of the pattern matching clause, \texttt{ys}, is checked against
\texttt{'a list($M$ + $N$)}, the result type for \texttt{rev}. This yields a
constraint $M + N = \texttt{n}$.

Thus, the first clause of the pattern matching in the function definition
generates the constraint:

$\forall n : nat. (0 + n = n)$

Analogously for the second case, \texttt{rev(x::xs, ys) = rev(xs, x::ys)}:

$\forall m : nat. \forall n : nat.(m + 1) + n = m + (n + 1)$,

Such constraints are then solved with a LIA-solver and if they are found
satisfiable then the type checking succeeds.

\subsubsection{Evaluation}

The use of the dependent types allows the compiler to know the length of all
properly type annotated lists and arrays in the program. Because of that, the
runtime array bound checks become redundant and the compiler may safely omit
them, which is what the \textbf{DML} compiler does.

The set of benchmarks consists of the programs taken from a few
\textbf{Standard ML} implementations standard libraries and adjusted for the
\textbf{DML}. The measurements display the considerable performance gain of
average 30\% among all programs.  These benefits come at the cost of changing
the type annotations to include dependent types which take about 17\% of lines
in the programs.

\subsection{Paper 2: Liquid Types}

\subsubsection{Problem}

The approach presented by the previous paper~\cite{Xi:1998:EAB} needs many manual type annotations.
This is tedious and sometimes it is not easy to come up with the correct dependent type.
Therefore Rondon et al. introduced logically qualified data types or short liquid types~\cite{Rondon:2008:LT}.
The advantage of liquid types is that those types can be inferred automatically.

\subsubsection{Solution}

Liquid types are dependent types where we restrict the refinements to conjunctions of predicates (qualifiers) and their negations.
This restriction is similar to predicate abstraction.
But unlike for predicate abstraction the set of predicates are chosen manually and not by examining the program code.
This choice depends on the property one wants to show by the analysis.
For example one can express types like $i:: \{\nu:$ \lstinline{int} $\mid 0 \le \nu \land \nu <$ \lstinline{len} $a\}$.
This type ensures that accessing array $a$ at index $i$ is safe.
Note that we used a predicate which compares the special value variable $\nu$ with the length of the array $a$.
To infer such types automatically 3 steps are executed:\medskip
\begin{enumerate}
    \item Step 1 performs a classical type inference as seen in class.
        The obtained types are transformed into templates where the unknown type refinements are expressed by so called liquid type variables.
    \item Step 2 generates a set of subtype constraints which restrict choices for the liquid type variables.
        This step is guided by the syntax.
    \item Step 3 tries to solve these constraints by embedding them into a decidable logic.
\end{enumerate}

\begin{lstlisting}[caption={Example Program},label=lst:exmpl,captionpos=t,float,abovecaptionskip=-\medskipamount]
let max x y =
    if x > y then x else y
\end{lstlisting}

To illustrate those 3 steps we look at an example in listing~\ref{lst:exmpl}.
In step 1 classical type inference yields the type $x:$ \lstinline{int} $\rightarrow y:$ \lstinline{int} $\rightarrow$ \lstinline{int} for \lstinline{max}.
Using this type we build the template $x: \kappa_x \rightarrow y: \kappa_y \rightarrow \kappa$.
We use $\kappa$ as abbreviation for $\{ \nu:$ \lstinline{int} $\mid \kappa\}$ and similar for $\kappa_x$ and $\kappa_y$.
The liquid type variables $\kappa_x, \kappa_y, \kappa$ are the unknown refinements for the arguments $x$ and $y$ plus the body of \lstinline{max}.
Step 2 generates the following constraints since the body is an \lstinline{if}-expression:
\begin{align}
    \label{algn:max}
    x: \kappa_x; y: \kappa_y; (x > y) \vdash \{\nu: \text{\lstinline|int|} \mid \nu = x \} <: \kappa\\
    x: \kappa_x; y: \kappa_y; \neg(x > y) \vdash \{\nu: \text{\lstinline|int|} \mid \nu = y \} <: \kappa
\end{align}
The first constraint forces the type of the \lstinline{then}-branch (namely $\{\nu:$ \lstinline{int} $\mid \nu = x \}$) to be a subtype
of the body ($\kappa$) in the environment where $x$ and $y$ have the appropriate types as well as the branch condition is true.
The second constraint is analogous for the \lstinline{else}-branch.
Note that in the environment the branch condition is false.
Step 3 translates these two constraints into a decidable logic and solves them using the predicates $P := \{x < \nu, x \le \nu, x = \nu, y < \nu, y \le \nu, y = \nu\}$.
First we set $\kappa_x$ and $\kappa_y$ to true because there are no constraints on $\kappa_x$ and $\kappa_y$.
This says that \lstinline{max} can be called by any integer.
For $\kappa$ we consider each case separately and build a conjunction of provable predicates and provable negated predicates.
This is again similar to predicate abstraction.
The first constraint simplifies then to $x > y \land \nu = x \Rightarrow \kappa$.
Therefore we obtain $\kappa := \neg(x < \nu) \land x \le \nu \land x = \nu \land y < \nu \land y \le \nu \land \neg(y = \nu)$ for the first constraint.
The second constraint simplifies to $\neg(x > y) \land \nu = y \Rightarrow \kappa$
which results in $\kappa := \neg(x < \nu) \land x \le \nu \land x = \nu \land y < \nu \land y \le \nu \land \neg(y = \nu)$.
By joining the information of both branches we have $\kappa := x \le \nu \land y \le \nu$.
Thus we obtain the type:
\begin{center}
    \lstinline{max} :: $x:$ \lstinline{int} $\rightarrow y:$ \lstinline{int} $\rightarrow \{\nu:$ \lstinline{int} $\mid x \le \nu \land y \le \nu\}$.
\end{center}

This example shows that the analysis is path sensitive by adding conditions to the environment.
The analysis can also handle recursion, higher-order functions and polymorphism.

\subsubsection{Evaluation}

In order to assess the advantage of this approach, liquid type inference is implemented in \textbf{DSolve}.
It is tested over a set of benchmarks from the \textbf{DML} project~\cite{Xi:1998:EAB}.
To allow array bounds checking, predicates which compare ($<, \le, =, \neq, \geq, >$) $\nu$ with 0, program variables and the length of arrays are used.
In most cases \textbf{DSolve} can prove the safety of array accesses without additional annotations, i.e. predicates.
Compared to the \textbf{DML} project the impact is very big.
Whereas in the \textbf{DML} project 17\% of all lines (31\% of characters) are annotations \textbf{DSolve} needs less than 1\% of lines and characters.


  \subsection{Refinement Types For Haskell}

    \subsubsection{Lazy Evaluation}

      Some programming languages, most notably Haskell, use a different
      evaluation strategy called lazy evaluation.  Intuitively, lazy evaluation
      means that computations are performed only if the results of those
      computations are needed.  For example, if a program specifies that the
      result of \texttt{div 10 5} be printed on screen, then this computation
      i.e. divide 10 by 5 will be performed.  Conversely, if the result of
      \texttt{div 10 5} is never needed, then the computation will never be
      performed.

      It turns out that classical approach on refinement types is unsound under
      lazy evaluation, in the sense that the typechecker will admit some bad
      behaviours.  Consider the following example.
      \begin{verbatim}
        type Pos = { v:Int | v >  0 }
        type Nat = { v:Int | v >= 0 }

        div :: n:Nat -> d:Pos -> { v:Nat | v < n } 
        (+) :: x:Int -> y:Int -> { v:Int | v = x + y }
        diverge :: Int -> { v:Nat | false } 

        explode :: Int -> Int
        explode x = let n = diverge 1 in 
                    let y = 0 in 
                    div x y
      \end{verbatim}

      When checking for the type of \texttt{y}, the typechecker will generate
      the constraint
      $$\mathtt{false}\wedge(y=0)\implies(v=0)\implies(v>0)$$
      which any SMT solver will accept since we have falsity in the antecedent.
      Note that the \texttt{false} part in the constraint comes from
      \texttt{diverge}, which essentially states that this function diverges
      i.e. will not return any value.

      This constraint is of course sound under eager evaluation, since we will
      never get into calling \texttt{div} with the second argument equals to
      zero i.e. division by zero.  Under lazy evaluation however, since
      \texttt{n} is not needed in the computation of the result of
      \texttt{explode}, the computation will jump straight into evaluating
      \texttt{div} thus resulting in division by zero.  Recall that the type of
      \texttt{div} should prevent this since it requires its second argument to
      have type \texttt{Pos}, a positive integer, which proves the unsoundness
      of the previous approach.

    \subsubsection{Solution}
      
      To restore soundness, the main idea is to label binders as potentially
      diverging and omit those in the typing constraint.  Returning to the
      previous example, by omitting \texttt{diverge} in the constraint, we have
      $$\mathtt{true}\wedge(y=0)\implies(v=0)\implies(v>0)$$
      which is now an invalid constraint and thus the typechecker will reject
      this as not well-typed.

      One way to implement this is to have a \emph{stratified} type system
      consisting of types which may diverge and those which may not.
      \begin{itemize}
        \item Initially, every binders have types which may diverge.  This is
          sound but very imprecise since now there is no constraint at all.
        \item Then termination analysis is used to determine which binders
          actually terminate, thus getting some constraints back.
      \end{itemize}

      Now it should be clear that the accuracy of this approach depends on the
      termination analysis.  Loosely speaking, there are three main points of
      it.
      \begin{itemize}
        \item As with previous approaches, the refinements itself must be
          restricted into some sort of decidable logic, because otherwise
          typechecking would be undecidable.  Here refinements are drawn from
          QF-EUFLIA, the decidable logic of equality, uninterpreted
          functions and linear arithmetic.
        \item To prove termination of inductively defined functions i.e.
          recursive functions, it employs some default size measures and check
          whether those measures decrease on each (recursive) function call.
          This approach is of course not unlike one found in other programming
          languages such as Coq. Using these default measures, termination can
          be proven automatically.
        \item If the automatic termination analysis fails, then the size
          measure must be explicitly specified by the user.
      \end{itemize}

    \subsubsection{Implementation and Result}

      LIQUIDHASKELL, a Haskell extension with refinement types succesfully
      brings refinement typechecking under lazy evaluation.  The termination
      analysis used in LIQUIDHASKELL is usable since it only requires about 1
      line of code of explicit size measure per 100 line of code, thus
      retaining the advantage of previous approach while maintaining soundness
      under lazy evaluation.

\section{Summary}

Simple type checking as seen in the class cannot capture some bad behaviours
that impose conditions on the types used in the program (for instance, simple
type annotations cannot express that a function should take integers only from
a certain range). The core idea of all three papers is to use dependent types
to make type systems more expressive and thus able to check more complex
program properties

As seen in the first paper, initial attempts to apply the dependent types for
the purpose of the static program analysis require comparatively many type
annotations to work.

Second paper describing Liquid Types presents the improvement on the previous
idea, being able to infer most annotations automatically.

Finally, the subject of the third paper, LIQUIDHASKELL, addresses the soundness
of Liquid Types under lazy evaluation while retaining the advantages of
automatic type inference.

All of the discussed type systems and ideas have ``real world'' implementations
as extensions of main-stream functional languages that confirm their usability
for certain analyses.

\section{Conclusion}

Dependent types allow a programmer to perform checking of certain run-time
program properties statically. However, the full richness of dependent types
comes at the cost of undecidability of the type checking. The shown
developments retain decidability and allow for a limited form of dependent
types to be used in the program.

Initial approach relied heavily on the help from the programmer, while the
later one, in form of Liquid Types, is able to infer most type information
without any help. Liquid type checking cannot be used under lazy evaluation
without any adaptation, but the adaptation is possible.

The analyses based on dependent types may be performed alongside or in place of
some traditional imprecise compiler optimizations or analyses (array bound
checks elimination, division by zero detection), making them precise. The
precision comes at the cost of the programmer's help for certain cases, but the
benefits may be invaluable in the application areas, where strong correctness
properties are needed.

%%
%% Bibliography
%%

%% Either use bibtex (recommended),

\bibliography{main}

%% .. or use the thebibliography environment explicitely



\end{document}
